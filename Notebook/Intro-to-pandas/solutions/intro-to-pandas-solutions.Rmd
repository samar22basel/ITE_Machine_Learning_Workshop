---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.10.2
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

<!-- #region slideshow={"slide_type": "slide"} -->
![](https://snag.gy/h9Xwf1.jpg)
<!-- #endregion -->

<!-- #region nbpresent={"id": "3433c1f4-195d-4a6b-8ad6-961d54c4c96d"} slideshow={"slide_type": "slide"} -->
<img src="https://maltem.com/wp-content/uploads/2020/04/LOGO_MALTEM.png" style="float: left; margin: 20px; height: 55px">

<br>
<br>
<br>
<br>

## Introduction to `pandas`

_Authors: Tim Book_

---

`pandas` is the most popular python package for managing datasets and is used extensively by data scientists.

### Learning Objectives

- Define the anatomy of DataFrames.
- Explore data with DataFrames.
- Practice plotting with pandas.
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "slide"} -->
### Lesson Guide

- What is `pandas`?
- Reading data
- Exploring data
    - Filtering
    - Sorting
- Split-Apply-Combine
- Missing Values
- Merging
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "slide"} -->
<a id='introduction'></a>

### What is `pandas`?

---

- Data analysis library - **Panel data system** (doesn't actually have to do with the animal, sorry).
- Created by Wes McKinney and Open Sourced by AQR Capital Management, LLC 2009.
- Implemented in highly optimized Python/Cython.
- Most ubiquitous tool used to start data analysis projects within the Python scientific ecosystem.

<!-- #endregion -->

<!-- #region slideshow={"slide_type": "slide"} -->
### Pandas Use Cases

---

- Cleaning data / Munging
- Exploratory Data Analysis (EDA)
- Structuring data for plots or tabular display
- Joining disparate sources
- Filtering, extracting, or transforming 
<!-- #endregion -->

## Importing the Dynamic Trio
From here on out, we'll begin pretty much all of our notebooks with the following three imports.

* **pandas**: The library we'll be using to do pretty much all data manipulation.
* **numpy**: The library we'll need to do various other computations. Even if you don't think you'll need it to start, you'll probably end up using it later.
* **matplotlib**: The library we'll use most for plotting. More on this another day.

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

### Discussion: Where do you think a data scientist spends most of their time?

`/poll "Where do you think a data scientist spends most of their time?" "Moving data" "Cleaning data" "Exploring data" "Plotting data" "Predictive modeling" anonymous limit 1`

<!-- #region nbpresent={"id": "b29851ac-daa2-46fb-90f0-125e0c4683e1"} slideshow={"slide_type": "slide"} -->
<a id='loading_csvs'></a>

### Loading a csv into a DataFrame

---

Pandas can load many types of files, but one of the most common filetypes for storing data is in a `.csv` file. Let's load a dataset on UFO sightings from the `./datasets` directory:
<!-- #endregion -->

```{python nbpresent={'id': '7e4ab5c8-5104-4004-9cd3-e781101e8703'}, slideshow={'slide_type': '-'}}
ufo = pd.read_csv('datasets/ufo.csv')
```

```{python}
type(ufo)
```

<!-- #region nbpresent={"id": "7c254479-e114-4b53-bb64-d9d5783d69ab"} slideshow={"slide_type": "fragment"} -->
This creates a pandas object called a **DataFrame**. These are powerful containers for data with many built-in functions to explore and manipulate data.

We will barely scratch the surface of DataFrame functionality in this lesson, but over the course of this class you will become an expert at using them.
<!-- #endregion -->

<!-- #region nbpresent={"id": "b44c04a3-0bd8-4961-a930-092e872469d0"} slideshow={"slide_type": "slide"} -->
<a id='loading_csvs'></a>

### Loading a csv into a DataFrame

---

Pandas can load many types of files, but one of the most common filetypes for storing data is in a `.csv` file. Let's load a dataset on UFO sightings from the `./datasets` directory:
<!-- #endregion -->

```{python nbpresent={'id': 'f332b800-d5b7-4ee9-8d36-77ecd7c4465c'}, slideshow={'slide_type': 'fragment'}}
ufo.head()
```

<!-- #region nbpresent={"id": "1e6c6b41-692a-43fd-b4e6-81ee125dad7e"} slideshow={"slide_type": "slide"} -->
If we want to see the last part of our data, we can equivalently use the ```.tail()``` function.
<!-- #endregion -->

```{python nbpresent={'id': '0077c2a2-48f2-4c30-9181-34d2fc466bc9'}, slideshow={'slide_type': '-'}}
ufo.tail()
```

<!-- #region nbpresent={"id": "2c901f72-6d15-4e6b-9592-de89ff0b4dfa"} slideshow={"slide_type": "slide"} -->
<a id='data_dimensions'></a>

### Data dimensions

---

It's good to look at what the dimensions of your data are. The ```.shape``` property will tell you the rows and colum counts of your DataFrame.
<!-- #endregion -->

```{python nbpresent={'id': '9a9b6f73-3b89-41e7-99eb-14e7afa83467'}, slideshow={'slide_type': '-'}}
ufo.shape
```

`/poll "In terms of rows, is this the largest dataset you've ever worked with?" "Yes" "No" "Not sure, but I'm not impressed anyway" anonymous limit 1`

You will notice that this is operates the same as `.shape` for numpy arrays/matricies. Pandas makes use of numpy under the hood for optimization and speed.

Look at the names of your columns with the ```.columns``` property.

```{python nbpresent={'id': 'b2da21bb-5a29-441d-a0b7-949ecf9938c8'}, slideshow={'slide_type': 'fragment'}}
ufo.columns
```

<!-- #region nbpresent={"id": "fc25fdf8-5959-4d0e-ab9d-ab6da5c83209"} slideshow={"slide_type": "slide"} -->
Accessing a specific column is easy. You can use the bracket syntax just like python dictionaries with the string name of the column to extract that column.
<!-- #endregion -->

```{python nbpresent={'id': 'c1d9b4d1-263d-4ca1-acf6-d18ad31c16e6'}, slideshow={'slide_type': 'fragment'}}
ufo['City'].head()
```

```{python}
# Try to refrain from doing this...
# THREAD: Why shouldn't you rely on this? (There are several good reasons).
ufo.City.head()
```

<!-- #region nbpresent={"id": "d1473cb4-9805-4831-aca8-71c68c271729"} slideshow={"slide_type": "fragment"} -->
As you can see we can also use the ```.head()``` function on a single column, which is represented as a pandas Series object.
<!-- #endregion -->

<!-- #region nbpresent={"id": "e5d6b43d-4118-465d-bea3-83a2caf96b8e"} slideshow={"slide_type": "slide"} -->
You can also access a column (as a DataFrame instead of a Series) or multiple columns with a list of strings.
<!-- #endregion -->

```{python nbpresent={'id': '46dc97d4-6326-417d-8745-64c49ef65ec2'}, slideshow={'slide_type': 'fragment'}}
ufo[['City', 'State']].head()
```

```{python}
ufo.head()
```

<!-- #region nbpresent={"id": "0c73f9d4-b242-441c-b606-40c05b450f25"} slideshow={"slide_type": "slide"} -->
<a id='dataframe_series'></a>

### DataFrame vs. Series

---

We've been playing with them, so I guess we should define them formally:

* A **`Series`** is a one-dimensional array of values **with an index**.
* A **`DataFrame`** is a two-dimensional array of values **with both a row and column index**.
* It turns out - each column of a `DataFrame` is actually a `Series`!

![](./assets/series-vs-df.png)

There is an important difference between using a list of strings and just a string with a column's name: when you use a list with the string it returns another **DataFrame**, but when you use just the string it returns a pandas **Series** object.
<!-- #endregion -->

```{python nbpresent={'id': 'a8526589-1116-4c85-be23-5cba34430f32'}, slideshow={'slide_type': 'fragment'}}
print(type(ufo['City']))

print(type(ufo[['City']]))
```

<!-- #region slideshow={"slide_type": "slide"} -->
<a id='info'></a>

### Examining your data with `.info()`

---

The `.info()` should be the first thing you look at when getting acquainted with a new dataset.

**Types** are very important.  They impact the way data will be represented in our machine learning models, how data can be joined, whether or not math operators can be applied, and when you can encounter unexpected results.

> _Typical problems when working with new datasets_:
> - Missing values
> - Unexpected types (string/object instead of int/float)
> - Dirty data (commas, dollar signs, unexpected characters, etc)
> - Blank values that are actually "non-null" or single white-space characters

`.info()` is a function that is available on every **DataFrame** object. It gives you information about:

- Name of column / variable attribute
- Type of index (RangeIndex is default)
- Count of non-null values by column / attribute
- Type of data contained in column / attribute
- Unqiue counts of dtypes (Pandas data types)
- Memory usage of our dataset

<!-- #endregion -->

```{python slideshow={'slide_type': 'slide'}}
ufo.info()
```

<!-- #region slideshow={"slide_type": "slide"} -->
## Aside:  Working with "Big Data"

---

The term **Big Data** has become a little bit of a buzzword with no clear, consensus definition. The most common definition is that **Big Data are data that are too big to fit in your computer's memory.**

![](https://snag.gy/UGNamo.jpg)

The reason that this definition is good is because when your data size exceeds your RAM, you have to use a separate set of tools to solve your problems. For example:

* Spark (Week 7!)
* Hadoop
* Being clever with how you read and use data
    - Separate it into small chunks for example.
<!-- #endregion -->

<!-- #region nbpresent={"id": "3f0c0568-5a2d-4559-ac13-7170a5c43846"} slideshow={"slide_type": "slide"} -->
<a id='describe'></a>

## Quick Summaries

---

The `.describe()` function is very useful for taking a quick look at your data. It gives you some of the basic descriptive statistics.

You can use `.value_counts()` to get a good tabular view of a categorical variable.
<!-- #endregion -->

```{python nbpresent={'id': '4d3ae9ac-b23b-4c49-9f0a-126c842419e8'}, slideshow={'slide_type': 'fragment'}}
# Let's read in the diamonds data set.
diamonds = pd.read_csv("datasets/diamonds.csv")
diamonds.head()
```

```{python}
# Let's describe the price
diamonds['price'].describe()
```

```{python nbpresent={'id': '59ba0259-fb6c-4bd1-b274-2035540493be'}}
# We can even do it to the whole DataFrame - what does that look like?
# What's missing?
diamonds.describe()
```

```{python}
# Let's count up the cuts
diamonds['cut'].value_counts()
```

```{python}
# Let's do the same thing, but normalized
diamonds['cut'].value_counts(normalize=True)
```

<!-- #region nbpresent={"id": "969967d8-7094-4960-8c4b-5583fed79648"} slideshow={"slide_type": "slide"} -->
```.describe()``` gives us these statistics:

- **count**, which is equivalent to the number of cells (rows)
- **mean**, the average of the values in the column
- **std**, which is the standard deviation
- **min**, the minimum value
- **25%**, the 25th percentile of the values 
- **50%**, the 50th percentile of the values, which is the equivalent to the median
- **75%**, the 75th percentile of the values
- **max**, the maximum value
<!-- #endregion -->

<!-- #region nbpresent={"id": "5bc97eed-d555-4e31-bbd6-d2f7bc26a1d4"} slideshow={"slide_type": "slide"} -->
There are built-in math functions that will work on all of the columns of a DataFrame at once, or subsets of the data.

I can use the `.mean()` function on the `ufo` DataFrame to get the mean for every column.
<!-- #endregion -->

```{python nbpresent={'id': 'adde643c-6657-4d32-bf3d-4406a9ae3303'}}
diamonds['price'].mean()
```

```{python}
diamonds['price'].median()
```

```{python}
diamonds['price'].quantile([0.025, 0.975])
```

<!-- #region slideshow={"slide_type": "slide"} -->
<a id='independent_practice'></a>

### Now you!

---

Now that we know a little bit about basic DataFrame use, let's practice on a new dataset.

> Pro tip:  You can use the "tab" key to browse filesystem resources when your cursor is in a string to get a relative reference to the files that can be loaded in Jupyter notebook.  Remember, you have to use your arrow keys to navigate the files populated in the UI. 

<img src="https://snag.gy/IlLNm9.jpg">

1. Read in the `cars.csv` dataset. (call it `cars`)
1. What is the mean `mpg` for cars in this dataset?
<!-- #endregion -->

```{python slideshow={'slide_type': 'fragment'}}
cars = pd.read_csv('datasets/cars.csv')
cars['mpg'].mean()
```

## Filtering
We usually don't need to operate on the _whole_ dataset. A very common task is to parse it down to only the pieces we need.

```{python}
cars.head()
```

```{python}
# What do you think the result of this cell is?
v = np.array([12, 98, 9, 50, 23])
v[[True, False, True, False, True]]
```

```{python}
# How about this?
v < 40
```

```{python}
# So...
v[v < 40]
```

```{python}
# And this?
cars['mpg'] > 30
```

```{python}
# Finally...
cars[cars['mpg'] > 30]
```

Filtering in pandas uses vectors of booleans to describe inclusion or exclusion. `True` means you're in, `False` means you're out.

```{python}
# This functions identically to the code above, and can sometimes feels a little cleaner.
# Variables that serve this function are sometimes called "masks"
high_mpg = cars['mpg'] > 30
cars[high_mpg]
```

### Multiple Filters
Often we want to filter based on multiple conditions. We can use the usual "and" and "or" logic, but the symbols change for mystical (read: annoying) Python reasons.

```{python}
# "And" logic - use ampersand (&)
# Note parentheses mandatory!
cars[(cars['cyl'] == 4) & (cars['am'] == 1)]
```

```{python}
# "Or" logic - use pipe (|)
cars[(cars['mpg'] < 14) | (cars['mpg'] > 30)]
```

### Now you:

```{python}
# (THREAD): Show me all the UFO sightings in your hometown! (City and State)
# Anything interesting?
ufo[(ufo['City'] == 'Towaco') & (ufo['State'] == 'NJ')]
```

### Aside: Some shortcuts

```{python}
cars[cars['mpg'].between(24, 30)]
```

```{python}
cars[~cars['mpg'].between(14, 31)]
```

```{python}
ufo[ufo['City'].isin(['Towaco', 'Montville'])]
```

<!-- #region nbpresent={"id": "5d655b68-e149-48cf-9739-b2f01fa13b88"} slideshow={"slide_type": "slide"} -->
<a id='indexing'></a>

## Pandas Indexing: `.loc` and `.iloc`

---

So far we've learned how to select both rows and columns. The savvy and skeptical student would have noticed a problem here. We have ambiguous notation! What does this do:

```python
data[something]
```

We can't tell! Is `something` a mask or a string? One selects rows, the other selects columns. **What if we wanted to filter rows and select columns at the same time?!**

Pandas has two properties that you can use for indexing:

- **`.loc`** indexes with the _labels_ for rows and columns axis.
- **`.iloc`** indexes with the _integer positions_ for rows and columns axis.
> There used to be a third, `.ix` which is now deprecated and shan't ever be used again.
<!-- #endregion -->

## `.loc` is Most Common
The syntax of `.loc` is pretty intuitive:

```python
data.loc[rows, columns]
```

Where `rows` is often a filter (ie, a **mask**), and `columns` is a list of columns, or even just `:` to select all columns.

```{python}
ufo.head()
```

```{python}
ufo.loc[ufo['State'] == 'TX', ['City', 'Shape Reported']].head()
```

```{python nbpresent={'id': '4162fb31-3624-4996-b1ec-a7a7eb23c5e5'}, slideshow={'slide_type': '-'}}
ufo.loc[ufo['State'] == 'TX', :].head()
```

### Acccctually.....
![](assets/actually.png)
According to **_The Zen of Python_**, explicit is better than implicit. `.loc` is explicit. **Most people choose to always use `.loc` instead of the ambiguous `data[something]` notation! This is a pretty good idea! When in doubt, use `.loc`!**

<!-- #region nbpresent={"id": "956f0a2a-51b8-4910-b220-3aa00d8e3848"} slideshow={"slide_type": "slide"} -->
### `.iloc` is rare, but useful
The `i` stands for "integer" and will give you the actual zero-indexed numerical indices.
<!-- #endregion -->

```{python nbpresent={'id': 'a59893a2-95e8-41cc-b5d9-e0a1fe0800d8'}}
cars.head()
```

```{python nbpresent={'id': 'dc4ccdda-2932-48c7-a7d7-1f1b1f30826c'}}
cars.iloc[:3, :]
```

```{python}
cars.iloc[:3, 3:6]
```

## Sorting

```{python}
# We can sort individual Series...
cars['mpg'].sort_values().head()
```

```{python}
cars['mpg'].sort_values(ascending=False).head()
```

```{python}
# Or the entire DataFrame
cars.sort_values('mpg').head()
```

### Now You:


```{python}
ufo.Time = pd.to_datetime(ufo.Time)
```

```{python}
# Give me the 5 most recent UFO sightings in Roswell, New Mexico.
# You'll need to filter and use .sort_values()
# This is a hard one!
ufo.loc[(ufo.State == 'NM') & (ufo.City == 'Roswell'), :].sort_values('Time', ascending=False).head()
```

<!-- #region nbpresent={"id": "b7034391-7d6d-4f08-beca-147e18036001"} slideshow={"slide_type": "slide"} -->
## Split-Apply-Combine

---

What if we want summary statistics _with respect to some categorical variable?_ For example, the price of a diamond probably varies widely between different diamond cuts. To tackle this problem, we'll use the **Split-Apply-Combine** technique. (This is sometimes called **MapReduce**, but is more of a special case of MapReduce). 

* **Split**: Separate your data into different DataFrames, one for each category.
* **Apply**: On each split-up DataFrame, apply some function or transformation (for example, the mean).
* **Combine**: Take the results and combine the split-up DataFrames back into one aggregate DataFrame.

This might sound complicated, but it's actually only two commands in pandas (the **Combine** step is done for us).
<!-- #endregion -->

```{python}
# What is the mean price by diamond cut?
diamonds.groupby('cut')['price'].mean()
```

```{python}
# Can we just describe each price by cut?
diamonds.groupby('cut')['price'].describe()
```

```{python nbpresent={'id': 'c42d85ec-a9a6-4880-84c5-1b73ace2341b'}}
# What if I want my own recipe of statistics?
diamonds.groupby('cut')['price'].agg(['count', 'mean', 'median'])
```

### Now You:
What is the mean miles per gallon for each cylinder size?

```{python}
cars.groupby('cyl')['mpg'].mean()
```

### Advanced Split-Apply-Combining
Feel free to skip!

```{python nbpresent={'id': '7e00119c-0641-4eaf-9432-dc3dffc68669'}}
# What if I want my own home-spun aggregate function?

# Maybe the mean of the log-price is interesting to you?
def log_mean(p):
    return np.mean(np.log(p))

diamonds.groupby('cut')['price'].agg(['count', 'mean', log_mean])
```

```{python}
# What if I want functions of different columns?
diamonds.groupby('cut').agg({
    'price': {'count': 'count', 'price_mean': 'mean'},
    'carat': {'carat_mean': 'mean'}
})
```

## Adding, Dropping, Renaming, and `inplace` Methods

```{python}
# Adding a column is easy, just define it!
# What if I wanted km per gal instead of miles per gal?
cars['kmpg'] = cars['mpg'] * 1.61
cars.head()
```

```{python}
# Oops - that actually doesn't make sense since they'd be using liters anyway.
# Let's drop it.
cars.drop('kmpg', axis=1).head()
```

```{python}
# But... it's not gone?
cars.head()
```

```{python}
cars.drop('kmpg', axis=1, inplace=True)
cars.head()
```

###  Inplace Methods!
There are several methods in pandas that don't "stick" unless you tell them to. These methods will always have `inplace=False` by default. If you want to run a method and have it "stick" - assign `inplace=True`.

For example...

### Renaming Columns

```{python}
# Yuck - I hate spaces and capital letters
ufo.head()
```

```{python}
# Lowercaseifying is easy:
# The "columns" attribute of a DataFrame works just like a numpy array or Series.
ufo.columns = ufo.columns.str.lower()
```

```{python}
ufo.head()
```

```{python}
# The .rename method
ufo.rename(columns={
    'colors reported': 'colors',
    'shape reported': 'shape'
}, inplace=True)
```

```{python}
ufo.head()
```

### Aside: `str` and `dt` methods
There are a lot of familiar string and date operations we can perform on columns. Strangely, they exist within a pandas submodule and so have to be prefixed with `str` and `dt` respectively.

```{python}
ufo['shape'].str.lower().head()
```

```{python}
ufo['shape'].str.replace('O', 'BRO').head()
```

```{python}
# We already did this above, but datetime variables need to be converted specially.
# ufo['time'] = pd.to_datetime(ufo['time'])
ufo['time'].dt.year.head()
```

## Missing Values

```{python}
s = pd.Series([5, 7, np.nan, 2, 10])
```

```{python}
s
```

```{python}
# Hmm...
s == np.nan
```

```{python}
s.isnull()
```

```{python}
s.notnull()
```

```{python}
ufo.isnull().head()
```

```{python}
ufo.isnull().sum()
```

```{python}
ufo.isnull().mean()
```

```{python}
# Easy way to filter out missings!
ufo.loc[ufo['colors'].notnull(), :].head()
```

## Exporting Data
We can read data, but how do we save it so we can send it out? pandas has several methods of the form `.to_*()`.

```{python}
cars.loc[cars['mpg'] > 30, :].to_csv('output/efficient-cars.csv', index=False)
```

<!-- #region slideshow={"slide_type": "slide"} -->
<a id='review'></a>

### Review

---

 - What would we do with a dataset when we first acquire it?
 - What's important to consider when first looking at a dataset? 
 - What are some common problems we can run into with new data?
 - What are some common operations with DataFrames?
 - How do we slice? Index? Filter?
<!-- #endregion -->

# EXTRA MATERIALS
![](assets/biohazard.png)
Everything that follows is considered advanced or "too much" for our first session with pandas, and may not be explicitly covered by the instructor. If the instructor _does_ cover it, please don't worry that you don't understand this on your first pass.

**THAT DOES NOT MEAN THESE TOPICS ARE UNIMPORTANT OR RARELY USED!** We highly _highly_ recommend you take a look at these on your own time.


### Merging

```{python}
movies = pd.read_csv(
    'datasets/movies.tbl',
    sep='|',
    encoding='latin1',
    header=None,
    names=['movie_id', 'title'],
    usecols=[0, 1]
)
movies.head()
```

```{python}
ratings = pd.read_csv(
    'datasets/movie_ratings.tsv',
    sep='\t',
    header=None,
    names=['user_id', 'movie_id', 'rating', 'timestamp']
)
ratings.head()
```

```{python}
movie_reviews = pd.merge(ratings, movies, how='left')
movie_reviews.head()
```

```{python}
print(movies.shape)
print(ratings.shape)
print(movie_reviews.shape)
```

### "Categorical" Variables
Despite the name, when pandas says "Categorical", they really mean "Ordinal" - that is, ordered categories.

For example, check out the following crosstab:

```{python}
pd.crosstab(diamonds['cut'], diamonds['color'])
```

The "cuts" are not in the right order! They're actually in alphabetical order. We can fix this by telling pandas that there really is an important ordering here.

```{python}
diamonds['cut'] = pd.Categorical(diamonds['cut'], categories=['Fair', 'Good', 'Very Good', 'Premium', 'Ideal'])
pd.crosstab(diamonds['cut'], diamonds['color'])
```

### Categorizing with `.map()`

```{python}
cars['cyl_word'] = cars['cyl'].map({4: 'Four', 6: 'Six', 8: 'Eight'})
cars['cyl_word'].value_counts()
```

```{python}
def is_efficient(x):
    if x > 20:
        return "Efficient"
    else:
        return "Wasteful"
    
cars['fuel_economy'] = cars['mpg'].map(is_efficient)
cars['fuel_economy'].value_counts()
```

### Advanced Data Manipulation with `.apply()`
The `.apply()` method is very similar to `.map()`, except more advanced. You can apply a function along any axis of a `DataFrame`. `.apply()` is our "Swiss army knife" for data manipulation - if something can't be solved with ordinary means, it might be time for a `.apply()`.

```{python}
sizes = pd.Series([8, 4, 5, 'L', 2, 12, 16, 8, 'XL'])
```

```{python}
def to_num(x):
    try:
        out = float(x)
    except:
        out = np.nan
    return out
```

```{python}
sizes.apply(to_num)
```

```{python}
cars.head()
```

```{python}
def describe_car(row):
    efficiency = row['fuel_economy'].lower()
    cyl = row['cyl_word'].lower()
    auto = 'automatic' if row['am'] == 1 else 'manual'
    print(f"This {cyl} cylinder car has {auto} transmission and a(n) {efficiency} fuel economy.")
```

```{python}
cars.head().apply(describe_car, axis=1)
```

BONUS QUESTION: Why are there 5 "None" valus in the above output?
